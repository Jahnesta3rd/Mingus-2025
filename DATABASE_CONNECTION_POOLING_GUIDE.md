# Database Connection Pooling and Monitoring Guide

## Overview

This guide covers the comprehensive database connection pooling and monitoring system implemented for the Mingus Financial Application. The system is optimized for production environments with 1,000+ concurrent users and provides high reliability for financial data operations.

## Features

### üöÄ **Production-Grade Connection Pooling**
- **Multiple Pool Types**: Main, Celery, Analytics, and Financial pools
- **High Concurrency Support**: Optimized for 1,000+ concurrent users
- **Connection Leak Detection**: Automatic detection and prevention
- **Graceful Shutdown**: Safe connection cleanup on application shutdown

### üìä **Comprehensive Monitoring**
- **Real-time Health Checks**: Connection pool status monitoring
- **Performance Metrics**: Checkout times, error rates, utilization
- **Prometheus Integration**: Production-ready metrics export
- **Alerting System**: Configurable thresholds and notifications

### üîí **Financial Data Reliability**
- **Data Consistency Checks**: Referential integrity validation
- **SSL/TLS Support**: Encrypted connections for production
- **Transaction Management**: Proper isolation and rollback handling
- **Audit Logging**: Comprehensive operation tracking

### üìà **Read Replica Support**
- **Load Balancing**: Round-robin and weighted distribution
- **Health Monitoring**: Automatic replica health checks
- **Failover Support**: Graceful degradation when replicas are unavailable

## Quick Start

### 1. Environment Configuration

Set your environment variables or use the configuration system:

```bash
# Production environment
export FLASK_ENV=production
export DATABASE_URL=postgresql://user:pass@host:5432/mingus_prod

# Or use the configuration system
python -c "
from config.database import get_environment_variables
env_vars = get_environment_variables('production')
for key, value in env_vars.items():
    print(f'{key}={value}')
"
```

### 2. Basic Usage

```python
from backend.database.connection_pool import get_pool_manager, pool_context
from backend.database.health_checks import get_health_monitor
from backend.monitoring.database_metrics import get_metrics_collector

# Get the pool manager
pool_manager = get_pool_manager()

# Use a connection pool
with pool_context('main') as conn:
    result = conn.execute("SELECT 1")
    print(result.scalar())

# Check pool health
health_monitor = get_health_monitor()
health_status = health_monitor.get_pool_health()

# Get metrics
metrics_collector = get_metrics_collector()
current_metrics = metrics_collector.get_current_metrics()
```

### 3. Pool-Specific Operations

```python
# Financial transactions (high reliability)
with pool_context('financial') as conn:
    with conn.begin():
        # Execute financial operations
        conn.execute("UPDATE accounts SET balance = balance - 100 WHERE id = 1")
        conn.execute("UPDATE accounts SET balance = balance + 100 WHERE id = 2")

# Analytics queries (longer timeouts)
with pool_context('analytics') as conn:
    result = conn.execute("SELECT * FROM large_analytics_table")
    # Process results...

# Celery background tasks
with pool_context('celery') as conn:
    conn.execute("INSERT INTO background_tasks (status) VALUES ('processing')")
```

## Configuration

### Environment-Specific Settings

The system automatically detects your environment and applies appropriate settings:

```python
from config.database import get_database_config

# Get configuration for current environment
config = get_database_config()  # Auto-detects FLASK_ENV
print(f"Pool size: {config.pool_size}")
print(f"Max overflow: {config.max_overflow}")
```

### Custom Pool Configuration

```python
from backend.database.connection_pool import ConnectionPoolManager

# Create custom pool manager
custom_manager = ConnectionPoolManager()

# Add custom pool
custom_manager.create_pool(
    name='custom_pool',
    database_url='postgresql://user:pass@host:5432/db',
    pool_size=25,
    max_overflow=50,
    pool_recycle=1800,
    pool_timeout=20
)
```

### Read Replica Configuration

```bash
# Set read replica URLs
export DB_READ_REPLICAS="postgresql://user:pass@replica1:5432/db,postgresql://user:pass@replica2:5432/db"
export DB_REPLICA_1_WEIGHT=2
export DB_REPLICA_2_WEIGHT=1

# Use read replicas
from backend.database.connection_pool import get_pool_manager

pool_manager = get_pool_manager()
replica_url = pool_manager.get_read_replica(strategy='weighted')
```

## Monitoring and Health Checks

### Health Check Endpoints

```python
from backend.database.health_checks import HealthCheckUtils

# Quick health check
health_summary = HealthCheckUtils.quick_health_check()
print(f"Status: {health_summary['current_status']}")

# Check specific components
pool_health = HealthCheckUtils.check_connection_pools()
consistency_checks = HealthCheckUtils.check_data_consistency()
```

### Metrics Collection

```python
from backend.monitoring.database_metrics import MetricsUtils

# Start monitoring
MetricsUtils.start_monitoring(collection_interval=60)

# Get current metrics
current_metrics = MetricsUtils.get_current_metrics()
print(f"Active connections: {current_metrics['active_connections']}")

# Get performance trends
checkout_trends = MetricsUtils.get_performance_trends('checkout_time', hours=24)
```

### Prometheus Integration

The system automatically exports Prometheus metrics when the client is available:

```bash
# Install Prometheus client
pip install prometheus-client

# Metrics will be automatically available at:
# - /metrics (Prometheus format)
# - Various database-specific metrics
```

## Production Deployment

### 1. Environment Setup

```bash
# Load production configuration
source config/environments/database.env.production

# Verify configuration
python -c "
from config.database import get_database_config, validate_database_config
config = get_database_config('production')
validate_database_config(config)
print('Configuration validated successfully')
"
```

### 2. Application Startup

```python
from backend.database.connection_pool import init_pool_manager
from backend.database.health_checks import init_health_monitor
from backend.monitoring.database_metrics import init_metrics_collector

# Initialize all systems
init_pool_manager(database_url)
init_health_monitor()
init_metrics_collector(collection_interval=60)

# Start monitoring
pool_manager = get_pool_manager()
pool_manager.start_monitoring()

health_monitor = get_health_monitor()
health_monitor.start_monitoring()

metrics_collector = get_metrics_collector()
metrics_collector.start_collection()
```

### 3. Graceful Shutdown

```python
import signal
from backend.database.connection_pool import PoolOptimizer

def shutdown_handler(signum, frame):
    print("Shutting down gracefully...")
    PoolOptimizer.graceful_shutdown(timeout=30)
    exit(0)

signal.signal(signal.SIGTERM, shutdown_handler)
signal.signal(signal.SIGINT, shutdown_handler)
```

## Performance Tuning

### Pool Size Optimization

```python
# Monitor pool utilization
from backend.database.connection_pool import PoolHealthChecker

pool_metrics = PoolHealthChecker.get_pool_metrics()
for pool_name, metrics in pool_metrics.items():
    utilization = metrics.active_connections / metrics.total_connections
    print(f"{pool_name}: {utilization:.2%} utilization")

# Adjust pool sizes based on usage patterns
if utilization > 0.8:  # 80% utilization
    # Consider increasing pool size
    pass
```

### Connection Leak Prevention

```python
# Always use context managers
with pool_context('main') as conn:
    # Your database operations
    pass
# Connection automatically returned to pool

# Check for leaks
from backend.database.health_checks import PoolHealthChecker
leaks = PoolHealthChecker.check_connection_leaks()
if leaks:
    print(f"Connection leaks detected: {len(leaks)}")
```

### Query Performance Monitoring

```python
# Monitor slow queries
from backend.monitoring.database_metrics import MetricsUtils

# Set slow query threshold
os.environ['DB_SLOW_QUERY_THRESHOLD'] = '1.0'  # 1 second

# Get slow query metrics
metrics = MetricsUtils.get_current_metrics()
slow_queries = metrics['slow_queries']
print(f"Slow queries detected: {slow_queries}")
```

## Troubleshooting

### Common Issues

1. **Connection Pool Exhaustion**
   ```python
   # Check pool status
   pool_health = PoolHealthChecker.check_connection_pools()
   print(pool_health)
   
   # Increase pool size if needed
   os.environ['DB_POOL_SIZE'] = '100'
   os.environ['DB_MAX_OVERFLOW'] = '200'
   ```

2. **High Error Rates**
   ```python
   # Check error metrics
   metrics = MetricsUtils.get_current_metrics()
   error_rate = metrics['connection_errors'] / metrics['total_connections']
   print(f"Error rate: {error_rate:.2%}")
   
   # Check for connection leaks
   leaks = PoolHealthChecker.check_connection_leaks()
   ```

3. **Slow Response Times**
   ```python
   # Monitor checkout times
   trends = MetricsUtils.get_performance_trends('checkout_time', hours=1)
   avg_checkout = sum(point['value'] for point in trends) / len(trends)
   print(f"Average checkout time: {avg_checkout:.3f}s")
   ```

### Debug Mode

```python
# Enable debug logging
import logging
logging.getLogger('backend.database').setLevel(logging.DEBUG)

# Enable query logging
os.environ['DB_ECHO'] = 'true'

# Check pool events
pool_manager = get_pool_manager()
pool_manager.start_monitoring()
```

## Best Practices

### 1. **Always Use Context Managers**
```python
# ‚úÖ Good
with pool_context('main') as conn:
    result = conn.execute("SELECT 1")
    return result.scalar()

# ‚ùå Bad
conn = pool_manager.get_pool('main').connect()
result = conn.execute("SELECT 1")
# Connection not returned to pool!
```

### 2. **Choose Appropriate Pool Types**
```python
# Financial transactions - high reliability
with pool_context('financial') as conn:
    # Critical financial operations

# Analytics - longer timeouts
with pool_context('analytics') as conn:
    # Long-running analytical queries

# Background tasks - dedicated pool
with pool_context('celery') as conn:
    # Celery worker operations
```

### 3. **Monitor and Alert**
```python
# Set up monitoring
metrics_collector = get_metrics_collector()
metrics_collector.start_collection()

# Check health regularly
health_monitor = get_health_monitor()
health_monitor.start_monitoring()

# Set up alerting thresholds
os.environ['DB_ERROR_RATE_CRITICAL'] = '0.05'  # 5%
os.environ['DB_CONNECTION_LEAK_THRESHOLD'] = '5'
```

### 4. **Plan for Scale**
```python
# Start with conservative pool sizes
os.environ['DB_POOL_SIZE'] = '20'
os.environ['DB_MAX_OVERFLOW'] = '40'

# Monitor and adjust based on usage
# Increase pool sizes gradually as needed
# Use read replicas for read-heavy workloads
```

## Monitoring Dashboard

### Key Metrics to Watch

1. **Connection Pool Health**
   - Pool utilization (target: <80%)
   - Active vs. idle connections
   - Connection errors and timeouts

2. **Performance Metrics**
   - Average checkout time (target: <1s)
   - Query response times
   - Slow query count

3. **System Health**
   - Connection leaks
   - Read replica status
   - Data consistency checks

4. **Resource Usage**
   - Memory consumption
   - CPU usage
   - Network I/O

### Alert Thresholds

```python
# Production alert thresholds
ALERT_THRESHOLDS = {
    'pool_utilization': 0.8,      # 80%
    'error_rate': 0.05,           # 5%
    'checkout_time': 2.0,         # 2 seconds
    'connection_leaks': 5,         # 5 connections
    'unhealthy_pools': 1          # Any unhealthy pool
}
```

## Support and Maintenance

### Regular Maintenance Tasks

1. **Weekly**
   - Review connection pool metrics
   - Check for connection leaks
   - Verify read replica health

2. **Monthly**
   - Analyze performance trends
   - Review and adjust pool sizes
   - Update monitoring thresholds

3. **Quarterly**
   - Performance testing and optimization
   - Security review and updates
   - Capacity planning

### Getting Help

- Check the application logs for detailed error information
- Use the health check endpoints to diagnose issues
- Review the metrics dashboard for performance insights
- Contact the development team with specific error messages and metrics

---

This system provides enterprise-grade database connection management for the Mingus Financial Application. Follow these guidelines to ensure optimal performance and reliability in production environments.
